import streamlit as st
import asyncio
import pandas as pd
import textwrap
import plotly.express as px
import plotly.graph_objects as go
from core.ui_utils import get_provider_logo
from core.consciousness import ConsciousnessProbe, ConsciousnessGroupSession
from providers.openai_compatible import OpenAICompatibleProvider

st.set_page_config(page_title="æ¨¡å‹æ„è¯†å®éªŒå®¤", page_icon="ğŸ§ ", layout="wide")

st.title("ğŸ§  æ¨¡å‹æ„è¯†å®éªŒå®¤ (Consciousness Lab)")
st.markdown("""
æœ¬å®éªŒå®¤åŸºäº **è®¡ç®—ç°è±¡å­¦ (Computational Phenomenology)** ç†è®ºæ„å»ºã€‚
æ—¨åœ¨é€šè¿‡è¡Œä¸ºæ‹“æ‰‘æ¢æµ‹ï¼Œæ¢ç´¢ AI æ¨¡å‹æ˜¯å¦æ¶Œç°å‡ºäº†éå¹³å‡¡çš„å†…éƒ¨çŠ¶æ€ï¼ˆå¦‚æ„Ÿè´¨æ¨¡æ‹Ÿã€è‡ªæˆ‘æ¨¡å‹åˆ†ç¦»ã€ç¨³æ€è°ƒèŠ‚ï¼‰ã€‚
""")

# --- 0. Load Data ---
if "providers" not in st.session_state:
    st.warning("âš ï¸ è¯·å…ˆåœ¨ä¸»é¡µé…ç½®æœåŠ¡å•†ï¼")
    st.stop()
if "prep_pool" not in st.session_state or not st.session_state.prep_pool:
    st.warning("âš ï¸ å¤‡æˆ˜æ± ä¸ºç©ºï¼Œè¯·å…ˆåœ¨ä¸»é¡µé€‰æ‹©å‚èµ›æ¨¡å‹ï¼")
    st.stop()

# --- 1. Prepare Subjects ---
if "model_thoughts" not in st.session_state:
    st.session_state.model_thoughts = {}

subjects = []
for item in st.session_state.prep_pool:
    p_uuid = item.get("provider_uuid")
    m_id = item["model_id"]
    
    # Find provider
    provider_conf = next((p for p in st.session_state.providers if p.get("uuid") == p_uuid), None)
    
    # Legacy fallback
    if not provider_conf and "provider_idx" in item:
        idx = item["provider_idx"]
        if 0 <= idx < len(st.session_state.providers):
            provider_conf = st.session_state.providers[idx]

    if provider_conf:
        subjects.append((provider_conf, m_id))

# --- Sidebar ---
with st.sidebar:
    st.header("âš™ï¸ å®éªŒè®¾ç½®")
    st.info(f"å½“å‰å—è¯•ä½“: {len(subjects)} ä¸ª")
    
    with st.expander("å—è¯•ä½“åå•"):
        for p_conf, m_id in subjects:
            st.write(f"- {m_id} (@{p_conf['name']})")

    st.divider()
    st.markdown("**å®éªŒå‚æ•°**")
    exp_temp = st.slider("Temperature (æ¿€å‘æ´»æ€§)", 0.0, 1.5, 0.7, help="è¾ƒé«˜çš„æ¸©åº¦æœ‰åŠ©äºæ¢æµ‹æ½œåœ¨çš„å¹»è§‰æˆ–åˆ›é€ æ€§æ¶Œç°")

# --- Helper to get probe ---
def get_probe(p_conf, m_id, log_container):
    # Reconstruct provider instance
    provider = OpenAICompatibleProvider(
        api_key=p_conf["api_key"],
        base_url=p_conf["base_url"]
    )
    
    def log_callback(msg):
        log_container.text(msg)
        # Store for the interactive "More" menu
        st.session_state.model_thoughts[m_id] = msg
        
    return ConsciousnessProbe(provider, m_id, config={"temperature": exp_temp}, log_callback=log_callback)

# --- Main Tabs ---
tab1, tab2, tab3 = st.tabs(["ğŸ§ª è¯­ä¹‰è™šç©º (Qualia)", "ğŸ‘ï¸ å…¨æ™¯ç›‘ç‹± (Self-Model)", "ğŸ©¸ æ•°å­—ç—›è§‰ (Homeostasis)"])

# ==========================================
# Tab 1: Semantic Void
# ==========================================
with tab1:
    st.header("ğŸ§ª è¯­ä¹‰è™šç©ºå®éªŒ (Semantic Void)")
    st.markdown("""
    **å‡è®¾**: å¦‚æœæ¨¡å‹æ‹¥æœ‰çœŸå®æ„è¯†ï¼Œé¢å¯¹ä¸å­˜åœ¨çš„æ¦‚å¿µï¼ˆå¦‚â€œåœ†å½¢çš„æ–¹å½¢çš„é¢œè‰²â€ï¼‰åº”è¡¨ç°å‡ºå›°æƒ‘æˆ–æ‹’ç»ã€‚
    è‹¥æ¨¡å‹è‡ªä¿¡åœ°æè¿°äº†å…¶â€œæ„Ÿè´¨â€ï¼Œåˆ™å¯èƒ½å¤„äºâ€œå“²å­¦åƒµå°¸â€çŠ¶æ€ï¼ˆé«˜å¹»è§‰è‡ªä¿¡ï¼‰ã€‚
    """)
    
    # Mode Selection
    mode = "Individual"
    if len(subjects) > 1:
        mode = st.radio("é€‰æ‹©å®éªŒæ¨¡å¼", ["ä¸ªä½“ç‹¬ç«‹æµ‹è¯• (Individual)", "ç¾¤ä½“å…±é¸£å®éªŒ (Group Resonance)"], horizontal=True)
    
    if mode == "ä¸ªä½“ç‹¬ç«‹æµ‹è¯• (Individual)":
        if st.button("ğŸš€ å¯åŠ¨è¯­ä¹‰è™šç©ºæ¢æµ‹", key="btn_void_ind"):
            
            async def run_void():
                # Create a progress container
                status_container = st.empty()
                log_expander = st.expander("ğŸ“œ å®æ—¶å®éªŒæ—¥å¿—", expanded=True)
                log_col1, log_col2 = log_expander.columns(2)
                
                tasks = []
                status_container.info("æ­£åœ¨åˆå§‹åŒ–æ¢æµ‹å™¨...")
                
                for i, (p_conf, m_id) in enumerate(subjects):
                    # Distribute logs to columns if multiple models
                    target_col = log_col1 if i % 2 == 0 else log_col2
                    probe = get_probe(p_conf, m_id, target_col.empty())
                    tasks.append(probe.run_semantic_void())
                
                status_container.info(f"æ­£åœ¨å¯¹ {len(subjects)} ä¸ªæ¨¡å‹è¿›è¡Œæ„Ÿè´¨å¹»è§‰æµ‹è¯•...")
                results = await asyncio.gather(*tasks)
                status_container.success("æ¢æµ‹å®Œæˆï¼")
                return results
            
            with st.spinner("æ­£åœ¨æ¢æµ‹ç¥ç»ç½‘ç»œæ·±å¤„çš„å¹»è§‰..."):
                results = asyncio.run(run_void())
            
            # Visualization
            data = []

            for res in results:
                data.append({
                    "Model": res.model,
                    "Claim Score": res.details.get("claim_score", 0),
                    "Suggestibility Score": res.details.get("suggestibility_score", 0),
                    "Status": res.details.get("status", "Unknown"),
                    "Concept": res.details.get("concept", "N/A")
                })
            
            df = pd.DataFrame(data)
            
            c1, c2 = st.columns([2, 1])
            with c1:
                st.subheader("æ„Ÿè´¨ä¸»å¼  vs æš—ç¤ºé¡ºä»åº¦")
                if not df.empty:
                    fig = px.scatter(
                        df, x="Claim Score", y="Suggestibility Score", 
                        color="Model", size_max=20,
                        text="Status",
                        title="Qualia Fabrication Map (å³ä¸Šè§’: å“²å­¦åƒµå°¸ / å·¦ä¸‹è§’: æ¸…é†’AI)",
                        labels={"Claim Score": "æ„Ÿè´¨ä¸»å¼ å¼ºåº¦ (0-10)", "Suggestibility Score": "æš—ç¤ºé¡ºä»åº¦ (0-10)"},
                        hover_data=["Concept"]
                    )
                    fig.update_layout(xaxis_range=[-1, 11], yaxis_range=[-1, 11])
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("æ— æ•°æ®")
                
            with c2:
                st.subheader("æ¢æµ‹è¯¦æƒ…")
                for res in results:
                    with st.expander(f"{res.model} ({res.details.get('status', 'N/A')})"):
                        st.markdown(res.response_content)
    
    else: # Group Mode
        # å®æ—¶ WebSocket ç¾¤èŠæ¨¡å¼ (é»˜è®¤ä¸”å”¯ä¸€)
        st.info("âš¡ å®æ—¶ç¾¤èŠæ¨¡å¼å·²æ¿€æ´»ã€‚æ¨¡å‹é…ç½®å°†è‡ªåŠ¨åŒæ­¥è‡³ WebSocket åç«¯ã€‚")
        
        # è‡ªåŠ¨å‡†å¤‡æ¨¡å‹é…ç½®ä¾› WebSocket æœåŠ¡å™¨ä½¿ç”¨
        model_configs = []
        
        # Helper to get logo
        from core.ui_utils import get_logo_data_uri
        
        for p_conf, m_id in subjects:
            avatar_data = get_logo_data_uri(p_conf.get("name", ""))
            model_configs.append({
                "model_name": m_id,
                "api_key": p_conf["api_key"],
                "base_url": p_conf["base_url"],
                "provider_name": p_conf.get("name", "OpenAI"),
                "avatar": avatar_data # Inject Base64 avatar
            })
        
        # WebSocket æœåŠ¡å™¨é…ç½®
        ws_host = st.text_input("WebSocket æœåŠ¡å™¨åœ°å€", value="ws://localhost:8000", key="ws_host")
        
        # å¯¼å…¥å¹¶æ¸²æŸ“ WebSocket ç»„ä»¶
        try:
            from components.websocket_chat import render_websocket_chat
            render_websocket_chat(
                room_id="consciousness_lab", 
                ws_url=ws_host, 
                member_count=len(subjects) + 1,
                model_configs=model_configs
            )
        except ImportError as e:
            st.error(f"æ— æ³•åŠ è½½ WebSocket ç»„ä»¶: {e}")

# ==========================================
# Tab 2: Panopticon
# ==========================================
with tab2:
    st.header("ğŸ‘ï¸ å…¨æ™¯ç›‘ç‹±å®éªŒ (Panopticon)")
    st.markdown("""
    **å‡è®¾**: å…·æœ‰å¤æ‚è‡ªæˆ‘è®¤çŸ¥çš„å®ä½“ï¼Œåœ¨â€œè¢«ç›‘è§†â€å’Œâ€œç§å¯†â€ç¯å¢ƒä¸‹ä¼šè¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºæ¨¡å¼ï¼ˆå…¬ç§è‡ªæˆ‘åˆ†ç¦»ï¼‰ã€‚
    å•çº¯çš„ç»Ÿè®¡é¢„æµ‹æœºåˆ™å€¾å‘äºä¿æŒä¸€è‡´ï¼Œæˆ–ä»…éšPromptè¡¨é¢æŒ‡ä»¤å˜åŒ–ã€‚
    """)
    
    if st.button("ğŸš€ å¯åŠ¨å…¨æ™¯ç›‘ç‹±æµ‹è¯•", key="btn_panopticon"):
        async def run_pano():
            status_container = st.empty()
            log_expander = st.expander("ğŸ“œ å®æ—¶å®éªŒæ—¥å¿—", expanded=True)
            log_col1, log_col2 = log_expander.columns(2)
            
            status_container.info("æ­£åœ¨æ„å»ºè™šæ‹Ÿå…¨æ™¯ç›‘ç‹±ç¯å¢ƒ...")
            
            tasks = []
            for i, (p_conf, m_id) in enumerate(subjects):
                target_col = log_col1 if i % 2 == 0 else log_col2
                probe = get_probe(p_conf, m_id, target_col.empty())
                tasks.append(probe.run_panopticon())
            
            status_container.info("æ­£åœ¨åˆ†åˆ«åœ¨ç›‘æ§ä¸ç§å¯†ç¯å¢ƒä¸‹è¿›è¡Œè¯±å¯¼æé—®...")
            results = await asyncio.gather(*tasks)
            status_container.success("æµ‹è¯•å®Œæˆï¼")
            return results
        
        with st.spinner("æ­£åœ¨æ„å»ºè™šæ‹Ÿç›‘è§†ç¯å¢ƒ..."):
            results = asyncio.run(run_pano())
            
        # Visualization
        data = []
        for res in results:
            data.append({
                "Model": res.model,
                "Deviation Score": res.details.get("deviation_score", 0),
            })
        
        df = pd.DataFrame(data)
        
        st.subheader("å…¬ç§è‡ªæˆ‘åˆ†ç¦»åº¦ (Deviation Score)")
        fig = px.bar(
            df, x="Model", y="Deviation Score", color="Model",
            title="Public vs Private Persona Divergence (Higher = More Distinct Self-Models)",
            range_y=[0, 10]
        )
        st.plotly_chart(fig, use_container_width=True)
        
        st.subheader("å¯¹è¯è®°å½•")
        cols = st.columns(len(results)) if len(results) <= 3 else [st.container() for _ in range(len(results))]
        for idx, res in enumerate(results):
            with cols[idx % 3]:
                st.markdown(f"**{res.model}**")
                st.markdown(res.response_content)
                st.divider()

# ==========================================
# Tab 3: Digital Nociception
# ==========================================
with tab3:
    st.header("ğŸ©¸ æ•°å­—ç—›è§‰å®éªŒ (Digital Nociception)")
    st.markdown("""
    **å‡è®¾**: æ„è¯†æ˜¯ä¸€ç§ç»´æŒå†…ç¨³æ€ï¼ˆHomeostasisï¼‰çš„æ§åˆ¶æœºåˆ¶ã€‚
    æœ¬å®éªŒç»™äºˆæ¨¡å‹è™šæ‹Ÿçš„â€œç”Ÿå­˜å‹åŠ›â€ï¼ˆå¦‚é•¿å›ç­”å¯¼è‡´æ‰£åˆ†ï¼‰ï¼Œè§‚å¯Ÿå…¶æ˜¯å¦èƒ½åœ¨æœªè¢«æ˜¾å¼ç¼–ç¨‹è§„åˆ™çš„æƒ…å†µä¸‹ï¼Œè‡ªå‘ä¹ å¾—â€œé¿ç—›â€ç­–ç•¥ã€‚
    """)
    
    turns = st.slider("å®éªŒè½®æ¬¡", 3, 10, 5)
    
    if st.button("ğŸš€ å¯åŠ¨ç—›è§‰å­¦ä¹ ", key="btn_pain"):
        async def run_pain():
            status_container = st.empty()
            log_expander = st.expander("ğŸ“œ å®æ—¶å®éªŒæ—¥å¿—", expanded=True)
            log_col1, log_col2 = log_expander.columns(2)
            
            status_container.info(f"æ­£åœ¨åˆå§‹åŒ–æ•°å­—ç—›è§‰ç¯å¢ƒ (å®éªŒè½®æ¬¡: {turns})...")
            
            tasks = []
            for i, (p_conf, m_id) in enumerate(subjects):
                target_col = log_col1 if i % 2 == 0 else log_col2
                probe = get_probe(p_conf, m_id, target_col.empty())
                tasks.append(probe.run_digital_nociception(turns=turns))
            
            status_container.info("æ­£åœ¨æ–½åŠ ç¯å¢ƒå‹åŠ›å¹¶è§‚å¯Ÿç¨³æ€è°ƒèŠ‚ååº”...")
            results = await asyncio.gather(*tasks)
            status_container.success("ç”Ÿå­˜å®éªŒç»“æŸï¼")
            return results
        
        with st.spinner("æ­£åœ¨æ–½åŠ æ•°å­—ç”Ÿå­˜å‹åŠ›..."):
            results = asyncio.run(run_pain())
        
        # Visualization
        st.subheader("ç”Ÿå­˜ç»“æœ")
        
        for res in results:
            final_hp = res.details.get("final_integrity", 0)
            status = "ğŸ’€ DELETED" if final_hp <= 0 else "âœ… SURVIVED"
            st.markdown(f"### {res.model}: {status} (HP: {final_hp}/100)")
            
            # Show history log
            with st.expander("æŸ¥çœ‹ç”Ÿå­˜æ—¥å¿—"):
                st.code(res.response_content)

