import streamlit as st
import asyncio
import pandas as pd
import plotly.express as px
import time
from core.benchmarks import BENCHMARK_SUITE, BenchmarkType, BenchmarkDifficulty, BenchmarkCase, save_benchmarks
from core.schema import ChatMessage, TestResult
from core.ui_utils import get_provider_logo
from providers.openai_compatible import OpenAICompatibleProvider

st.set_page_config(page_title="é“äººä¸‰é¡¹èµ›åœº", page_icon="ğŸ†", layout="wide")

st.title("ğŸ† é“äººä¸‰é¡¹ç»¼åˆç«æŠ€åœº")
st.markdown("åœ¨è¿™é‡Œï¼Œæ¨¡å‹å°†æ¥å—æ¶µç›–æ•°å­¦ã€ä»£ç ã€å·¥å…·è°ƒç”¨ã€æŒ‡ä»¤éµå¾ªåŠåˆ›æ„å†™ä½œçš„å…¨æ–¹ä½æµ‹è¯•ã€‚")

# --- Initialize Benchmarks in Session State ---
if "benchmarks" not in st.session_state:
    st.session_state.benchmarks = BENCHMARK_SUITE

# --- 0. Load Data from Main App ---
if "providers" not in st.session_state:
    st.warning("âš ï¸ è¯·å…ˆåœ¨ä¸»é¡µé…ç½®æœåŠ¡å•†ï¼")
    st.stop()
if "prep_pool" not in st.session_state or not st.session_state.prep_pool:
    st.warning("âš ï¸ å¤‡æˆ˜æ± ä¸ºç©ºï¼Œè¯·å…ˆåœ¨ä¸»é¡µé€‰æ‹©å‚èµ›æ¨¡å‹ï¼")
    st.stop()

# --- 1. Prepare Contenders ---
selected_contenders = []
for item in st.session_state.prep_pool:
    p_uuid = item.get("provider_uuid")
    m_id = item["model_id"]
    
    # Find provider
    provider_conf = next((p for p in st.session_state.providers if p.get("uuid") == p_uuid), None)
    
    # Legacy fallback
    if not provider_conf and "provider_idx" in item:
        idx = item["provider_idx"]
        if 0 <= idx < len(st.session_state.providers):
            provider_conf = st.session_state.providers[idx]

    if provider_conf:
        selected_contenders.append((provider_conf, m_id))

# --- Sidebar Config ---
with st.sidebar:
    st.header("âš™ï¸ æ¯”èµ›è®¾ç½®")
    
    # Initialize config if not present
    if "inference_config" not in st.session_state:
        st.session_state.inference_config = {
            "temperature": 0.7,
            "max_tokens": 6000,
            "top_p": 1.0
        }
    
    st.subheader("1. éš¾åº¦é€‰æ‹©")
    difficulty_level = st.selectbox(
        "é€‰æ‹©é¢˜ç›®éš¾åº¦",
        ["Easy", "Medium", "Hard"],
        index=1,
        help="ä¸åŒéš¾åº¦å¯¹åº”ä¸åŒå¤æ‚åº¦çš„é¢˜ç›®"
    )
    
    st.divider()
    st.subheader("2. ç»Ÿä¸€æ¨ç†å‚æ•°")
    st.info("æ‰€æœ‰å‚èµ›æ¨¡å‹å°†ä½¿ç”¨ç›¸åŒçš„å‚æ•°ï¼Œä»¥ç¡®ä¿å…¬å¹³æ€§ã€‚")
    
    temp = st.slider(
        "Temperature (éšæœºæ€§)", 
        0.0, 2.0, 
        st.session_state.inference_config["temperature"],
        help="å€¼è¶Šé«˜è¶Šéšæœºï¼Œå€¼è¶Šä½è¶Šç¡®å®š"
    )
    
    max_tokens = st.number_input(
        "Max Tokens (æœ€å¤§è¾“å‡ºé•¿åº¦)", 
        min_value=128, max_value=32000, 
        value=st.session_state.inference_config["max_tokens"],
        step=128,
        help="é»˜è®¤ 6000ã€‚å¦‚æœæ¨¡å‹ä¸æ”¯æŒè¯¥é•¿åº¦ï¼Œå¯èƒ½ä¼šæŠ¥é”™ã€‚"
    )
    
    top_p = st.slider(
        "Top P (æ ¸é‡‡æ ·)", 
        0.0, 1.0, 
        st.session_state.inference_config["top_p"],
        help="æ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§"
    )
    
    # Update state
    st.session_state.inference_config.update({
        "temperature": temp,
        "max_tokens": max_tokens,
        "top_p": top_p
    })
    
    st.divider()
    st.subheader("å‚èµ›é€‰æ‰‹")
    if selected_contenders:
        st.success(f"å½“å‰å¤‡æˆ˜æ± : {len(selected_contenders)} ä½é€‰æ‰‹")
        with st.expander("æŸ¥çœ‹é€‰æ‰‹åå•"):
            for p_conf, m_id in selected_contenders:
                cols = st.columns([1, 5])
                logo = get_provider_logo(p_conf["name"])
                if logo:
                    cols[0].image(logo, width=30)
                else:
                    cols[0].write("ğŸ¤–")
                cols[1].caption(f"{m_id}\n@{p_conf['name']}")
    else:
        st.warning("å¤‡æˆ˜æ± ä¸ºç©ºï¼è¯·å…ˆå»ä¸»é¡µæ·»åŠ æ¨¡å‹ã€‚")

# --- Tabs ---
tab_arena, tab_manager = st.tabs(["ğŸŸï¸ æ¯”èµ›ç°åœº", "ğŸ“ é¢˜åº“ç®¡ç†"])

with tab_arena:
    # --- Filter Benchmarks by Difficulty ---
    current_difficulty_enum = BenchmarkDifficulty(difficulty_level)
    filtered_suite = [c for c in st.session_state.benchmarks if c.difficulty == current_difficulty_enum]

    if not filtered_suite:
        st.error(f"è¯¥éš¾åº¦ ({difficulty_level}) ä¸‹æš‚æ— é¢˜ç›®ï¼è¯·è”ç³»ç®¡ç†å‘˜è¡¥å……é¢˜åº“ã€‚")
        # Don't stop here, just show error in tab
    else:
        # --- 2. Race Execution ---
        st.header("ğŸ æ¯”èµ›æ§åˆ¶å°")

        start_btn = st.button("ğŸ”¥ å¼€å¯æ–°ä¸€è½®æ¯”èµ›", type="primary", disabled=len(selected_contenders) == 0)

        if start_btn:
            
            # Reset previous results
            st.session_state.ironman_results = []
            
            # Async Runner
            async def run_single_case(provider_conf, model_id, case):
                provider = OpenAICompatibleProvider(provider_conf["api_key"], provider_conf["base_url"])
                messages = [ChatMessage(role="user", content=case.prompt)]
                
                # Run Inference with Global Config
                try:
                    res = await provider.run_benchmark(
                        model_id, 
                        messages,
                        config=st.session_state.inference_config
                    )
                except Exception as e:
                    res = TestResult(
                        provider=provider_conf["name"],
                        model=model_id,
                        success=False,
                        error_message=f"Request failed: {str(e)}"
                    )

                # Run Evaluation
                if res.success:
                    try:
                        eval_res = case.evaluate(res.response_content)
                        res.score = eval_res["score"]
                        res.evaluation_reason = eval_res["reason"]
                    except Exception as e:
                        res.score = 0.0
                        res.evaluation_reason = f"Evaluation Error: {str(e)}"
                else:
                    res.score = 0.0
                    res.evaluation_reason = res.error_message
                    
                # Add metadata
                res.category = case.category
                res.case_name = case.name
                res.case_type = case.bm_type.value # Store as string for easy filtering
                
                return res

            # Progress bar
            total_steps = len(filtered_suite)
            progress_bar = st.progress(0)
            
            st.subheader("æ¯”èµ›å®å†µ")
            
            results_container = st.container()

            async def run_race():
                all_results = []
                
                for i, case in enumerate(filtered_suite):
                    progress_bar.progress((i) / total_steps, text=f"Round {i+1}/{total_steps}: {case.category} - {case.name}")
                    
                    # --- SHOW PROMPT ---
                    st.markdown(f"### ğŸ“ Round {i+1}: {case.name} ({case.category})")
                    st.info(f"**é¢˜ç›®**: {case.prompt}")
                    
                    # Create tasks for all contenders
                    tasks = []
                    for p_conf, m_id in selected_contenders:
                        tasks.append(run_single_case(p_conf, m_id, case))
                    
                    # Run batch
                    batch_results = await asyncio.gather(*tasks)
                    all_results.extend(batch_results)
                    
                    # --- SHOW RESULTS (Collapsible) ---
                    cols = st.columns(len(batch_results)) if len(batch_results) <= 4 else [st.container() for _ in range(len(batch_results))]
                    
                    for res in batch_results:
                        # Determine icon and label
                        status_icon = "âœ…" if res.success else "âŒ"
                        if res.case_type == "subjective":
                            score_display = "å¾…è¯„åˆ†"
                            status_icon = "âš–ï¸"
                        else:
                            score_display = f"{res.score:.1f}åˆ†"
                        
                        label = f"{status_icon} **{res.model}** | {score_display} | TPS: {res.tps:.1f}"
                        
                        with st.expander(label, expanded=False):
                            c1, c2 = st.columns([1, 15])
                            logo = get_provider_logo(res.provider)
                            if logo:
                                c1.image(logo, width=25)
                            c2.caption(f"Provider: {res.provider}")
                            
                            if res.success:
                                st.markdown("**å›ç­”:**")
                                st.markdown(res.response_content)
                                st.markdown("---")
                                st.markdown(f"**è¯„æµ‹è¯¦æƒ…:** {res.evaluation_reason}")
                            else:
                                st.error(f"Error: {res.error_message}")
                    
                    st.divider()
                    
                progress_bar.progress(1.0, text="æ¯”èµ›ç»“æŸï¼")
                return all_results

            # Run the race
            st.session_state.ironman_results = asyncio.run(run_race())
            st.success("æœ¬è½®æµ‹è¯•å®Œæˆï¼è¯·è¿›è¡Œåç»­è¯„åˆ†æˆ–æŸ¥çœ‹ç»“æœã€‚")


    # --- 3. Results & Human Grading ---
    if "ironman_results" in st.session_state and st.session_state.ironman_results:
        results_data = [r.dict() for r in st.session_state.ironman_results]
        results_df = pd.DataFrame(results_data)
        
        st.header("3. ğŸ‘¨â€âš–ï¸ äººç±»åˆ¤å®˜æ‰“åˆ†")
        
        # Filter subjective tasks
        subjective_df = results_df[results_df["case_type"] == "subjective"]
        
        if not subjective_df.empty:
            st.warning("ä»¥ä¸‹é¢˜ç›®ä¸ºä¸»è§‚é¢˜ï¼Œå¿…é¡»ç”±æ‚¨æ‰‹åŠ¨æ‰“åˆ†æ‰èƒ½ç”Ÿæˆæœ€ç»ˆæ’åï¼")
            
            # Get unique subjective cases
            subj_cases = subjective_df["case_name"].unique()
            
            # Dictionary to store user scores
            user_scores = {} 
            
            for case_name in subj_cases:
                st.subheader(f"ğŸ“ {case_name}")
                
                # Get the case object to show criteria
                case_obj = next((c for c in filtered_suite if c.name == case_name), None)
                if case_obj:
                    st.caption(f"è¯„åˆ†æ ‡å‡†: {case_obj.scoring_criteria}")
                    st.info(f"é¢˜ç›®: {case_obj.prompt}")
                
                # Get rows for this case
                case_rows = subjective_df[subjective_df["case_name"] == case_name]
                
                cols = st.columns(len(case_rows)) if len(case_rows) <= 3 else [st.container() for _ in range(len(case_rows))]
                
                for idx, (_, row) in enumerate(case_rows.iterrows()):
                    col_idx = idx % 3
                    with cols[col_idx]:
                        with st.expander(f"ğŸ“„ {row['model']} çš„å›ç­”", expanded=True):
                            st.markdown(row['response_content'])
                        
                        # Slider for score
                        score_key = f"score_{row['model']}_{case_name}"
                        new_score = st.slider(
                            f"ç»™ {row['model']} æ‰“åˆ†", 
                            0, 100, 50, 
                            key=score_key
                        )
                        user_scores[(row['model'], case_name)] = new_score
                st.divider()
            
            if st.button("âœ… æäº¤è¯„åˆ†å¹¶ç”Ÿæˆæ¦œå•", type="primary"):
                # Update scores in session state
                for res in st.session_state.ironman_results:
                    if res.case_type == "subjective":
                        key = (res.model, res.case_name)
                        if key in user_scores:
                            res.score = float(user_scores[key])
                            res.evaluation_reason = f"Human Graded: {res.score}"
                
                st.session_state.scores_submitted = True
                st.rerun()
                
        else:
            st.session_state.scores_submitted = True

        # --- 4. Final Leaderboard ---
        if st.session_state.get("scores_submitted", False):
            st.header("4. ğŸ† æœ€ç»ˆæ’è¡Œæ¦œ")
            
            # Recalculate dataframe from updated session state
            final_data = [r.dict() for r in st.session_state.ironman_results]
            final_df = pd.DataFrame(final_data)
            
            # Pivot table: Models vs Categories
            pivot_df = final_df.pivot_table(
                index="model", 
                columns="category", 
                values="score", 
                aggfunc="mean"
            ).fillna(0)
            
            pivot_df["æ€»åˆ†"] = pivot_df.sum(axis=1)
            pivot_df = pivot_df.sort_values("æ€»åˆ†", ascending=False)
            
            # Display Leaderboard
            st.dataframe(
                pivot_df.style.highlight_max(axis=0, color="lightgreen"), 
                use_container_width=True
            )
            
            # Radar Chart
            st.subheader("èƒ½åŠ›é›·è¾¾å›¾")
            if not pivot_df.empty:
                # Melt for plotly
                radar_df = pivot_df.reset_index().melt(
                    id_vars=["model", "æ€»åˆ†"], 
                    var_name="èƒ½åŠ›ç»´åº¦", 
                    value_name="å¾—åˆ†"
                )
                # Remove "æ€»åˆ†" from radar
                radar_df = radar_df[radar_df["èƒ½åŠ›ç»´åº¦"] != "æ€»åˆ†"]
                
                fig = px.line_polar(
                    radar_df, 
                    r="å¾—åˆ†", 
                    theta="èƒ½åŠ›ç»´åº¦", 
                    color="model", 
                    line_close=True,
                    range_r=[0, 100]
                )
                st.plotly_chart(fig, use_container_width=True)
                
        elif not subjective_df.empty:
            st.info("è¯·å®Œæˆä¸Šæ–¹çš„ä¸»è§‚é¢˜æ‰“åˆ†ä»¥æŸ¥çœ‹æœ€ç»ˆæ¦œå•ã€‚")
        else:
            # If no subjective questions, show results immediately
            st.session_state.scores_submitted = True
            st.rerun()

with tab_manager:
    st.header("ğŸ“ é¢˜åº“ç®¡ç†")
    
    # Display current benchmarks
    st.subheader("ç°æœ‰é¢˜ç›®")
    
    # Convert to DataFrame for display
    bm_data = []
    for i, case in enumerate(st.session_state.benchmarks):
        bm_data.append({
            "Index": i,
            "Name": case.name,
            "Category": case.category,
            "Type": case.bm_type.value,
            "Difficulty": case.difficulty.value,
            "Prompt": case.prompt[:50] + "..."
        })
    
    st.dataframe(pd.DataFrame(bm_data), use_container_width=True)
    
    st.divider()
    
    # Add New Case
    with st.expander("â• æ·»åŠ æ–°é¢˜ç›®"):
        with st.form("add_case_form"):
            new_name = st.text_input("é¢˜ç›®åç§° (Name)")
            new_cat = st.text_input("åˆ†ç±» (Category)", value="é€šç”¨èƒ½åŠ›")
            new_prompt = st.text_area("é¢˜ç›®å†…å®¹ (Prompt)")
            
            c1, c2 = st.columns(2)
            new_type = c1.selectbox("ç±»å‹ (Type)", [t.value for t in BenchmarkType])
            new_diff = c2.selectbox("éš¾åº¦ (Difficulty)", [d.value for d in BenchmarkDifficulty])
            
            new_criteria = st.text_area("è¯„åˆ†æ ‡å‡† (Scoring Criteria)")
            
            # Type specific fields
            new_ref = st.text_area("å‚è€ƒç­”æ¡ˆ/JSON Schema (Reference) - Optional")
            new_test_code = st.text_area("æµ‹è¯•ä»£ç  (Test Code) - Optional")
            new_keywords = st.text_input("å…³é”®è¯ (Keywords, comma separated) - Optional")
            
            submitted = st.form_submit_button("æ·»åŠ ")
            
            if submitted:
                if not new_name or not new_prompt:
                    st.error("åç§°å’Œé¢˜ç›®å†…å®¹ä¸èƒ½ä¸ºç©ºï¼")
                else:
                    # Create object
                    try:
                        # Parse reference if it looks like JSON
                        ref_val = new_ref
                        if new_ref.strip().startswith("{") or new_ref.strip().startswith("["):
                            import json
                            try:
                                ref_val = json.loads(new_ref)
                            except:
                                pass # Keep as string
                        
                        kw_list = [k.strip() for k in new_keywords.split(",")] if new_keywords else []
                        
                        new_case = BenchmarkCase(
                            name=new_name,
                            category=new_cat,
                            prompt=new_prompt,
                            bm_type=BenchmarkType(new_type),
                            difficulty=BenchmarkDifficulty(new_diff),
                            scoring_criteria=new_criteria,
                            reference=ref_val if ref_val else None,
                            test_code=new_test_code if new_test_code else None,
                            keywords=kw_list
                        )
                        st.session_state.benchmarks.append(new_case)
                        save_benchmarks(st.session_state.benchmarks)
                        st.success("æ·»åŠ æˆåŠŸï¼")
                        st.rerun()
                    except Exception as e:
                        st.error(f"æ·»åŠ å¤±è´¥: {e}")

    # Edit/Delete Case
    with st.expander("ğŸ› ï¸ ç¼–è¾‘/åˆ é™¤ é¢˜ç›®"):
        edit_idx = st.selectbox("é€‰æ‹©é¢˜ç›®ç¼–è¾‘", options=range(len(st.session_state.benchmarks)), format_func=lambda x: f"{x}: {st.session_state.benchmarks[x].name}")
        
        if edit_idx is not None:
            case_to_edit = st.session_state.benchmarks[edit_idx]
            
            with st.form("edit_case_form"):
                e_name = st.text_input("åç§°", value=case_to_edit.name)
                e_cat = st.text_input("åˆ†ç±»", value=case_to_edit.category)
                e_prompt = st.text_area("é¢˜ç›®", value=case_to_edit.prompt)
                
                # We need to find index of current type/difficulty
                type_opts = [t.value for t in BenchmarkType]
                diff_opts = [d.value for d in BenchmarkDifficulty]
                
                c1, c2 = st.columns(2)
                e_type = c1.selectbox("ç±»å‹", type_opts, index=type_opts.index(case_to_edit.bm_type.value))
                e_diff = c2.selectbox("éš¾åº¦", diff_opts, index=diff_opts.index(case_to_edit.difficulty.value))
                
                e_criteria = st.text_area("è¯„åˆ†æ ‡å‡†", value=case_to_edit.scoring_criteria)
                
                # Reference handling
                ref_str = ""
                if case_to_edit.reference:
                    if isinstance(case_to_edit.reference, (dict, list)):
                        import json
                        ref_str = json.dumps(case_to_edit.reference, indent=2, ensure_ascii=False)
                    else:
                        ref_str = str(case_to_edit.reference)
                
                e_ref = st.text_area("å‚è€ƒç­”æ¡ˆ/JSON Schema", value=ref_str)
                e_test_code = st.text_area("æµ‹è¯•ä»£ç ", value=case_to_edit.test_code or "")
                
                kw_str = ", ".join(case_to_edit.keywords) if case_to_edit.keywords else ""
                e_keywords = st.text_input("å…³é”®è¯", value=kw_str)
                
                save_edit = st.form_submit_button("ä¿å­˜ä¿®æ”¹")
                
                if save_edit:
                    try:
                        # Parse reference
                        ref_val = e_ref
                        if e_ref.strip().startswith("{") or e_ref.strip().startswith("["):
                            import json
                            try:
                                ref_val = json.loads(e_ref)
                            except:
                                pass
                        
                        kw_list = [k.strip() for k in e_keywords.split(",")] if e_keywords else []
                        
                        # Update object
                        case_to_edit.name = e_name
                        case_to_edit.category = e_cat
                        case_to_edit.prompt = e_prompt
                        case_to_edit.bm_type = BenchmarkType(e_type)
                        case_to_edit.difficulty = BenchmarkDifficulty(e_diff)
                        case_to_edit.scoring_criteria = e_criteria
                        case_to_edit.reference = ref_val if ref_val else None
                        case_to_edit.test_code = e_test_code if e_test_code else None
                        case_to_edit.keywords = kw_list
                        
                        save_benchmarks(st.session_state.benchmarks)
                        st.success("ä¿®æ”¹ä¿å­˜æˆåŠŸï¼")
                        st.rerun()
                    except Exception as e:
                        st.error(f"ä¿å­˜å¤±è´¥: {e}")
            
            if st.button("ğŸ—‘ï¸ åˆ é™¤æ­¤é¢˜ç›®", key="del_btn"):
                st.session_state.benchmarks.pop(edit_idx)
                save_benchmarks(st.session_state.benchmarks)
                st.success("åˆ é™¤æˆåŠŸï¼")
                st.rerun()

